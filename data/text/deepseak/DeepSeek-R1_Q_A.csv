user_input,reference_contexts,reference,synthesizer_name
How has OpenAI contributed to advancements in reasoning capabilities of Large Language Models (LLMs) in recent research?,"[""Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##""]","OpenAI has significantly contributed to advancements in reasoning capabilities of Large Language Models (LLMs) through its o1 series models, which were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has led to substantial improvements in various reasoning tasks, including mathematics, coding, and scientific reasoning. Despite the challenges of effective test-time scaling, OpenAI's models have demonstrated general reasoning performance that surpasses other methods explored in the research community.",single_hop_specifc_query_synthesizer
Can you explain how DeepSeek-R1-Zero improves reasoning capabilities in language models?,"[""Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##""]","DeepSeek-R1-Zero emerged during the training process and exhibits super performance on reasoning benchmarks, with a pass@1 score on AIME 2024 increasing from 15.6% to 71.0%, and further improving to 86.7% with majority voting, matching the performance of OpenAI-o1-0912. It utilizes pure reinforcement learning (RL) to enhance reasoning capabilities without any supervised data, focusing on self-evolution through the RL process. However, it faces challenges such as poor readability and language mixing.",single_hop_specifc_query_synthesizer
What is GRPO in the context of AI research?,"[""Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##""]","GRPO is the reinforcement learning framework used to improve model performance in reasoning, specifically in the context of enhancing the reasoning capabilities of language models without any supervised data.",single_hop_specifc_query_synthesizer
What advancements has Google made in Large Language Models recently?,"[""Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##""]","Google has been involved in the rapid iteration and evolution of Large Language Models (LLMs), progressively diminishing the gap towards Artificial General Intelligence (AGI). Their recent work includes the introduction of post-training as an important component of the full training pipeline, which enhances accuracy on reasoning tasks and aligns with social values while requiring minimal computational resources.",single_hop_specifc_query_synthesizer
What Large Language Models do for reasoning?,"[""Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##""]","Large Language Models (LLMs) have been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. Specifically, OpenAI's o1 series models introduced inference-time scaling, which has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning.",single_hop_specifc_query_synthesizer
What advancements does DeepSeek-R1-Zero bring to reasoning capabilities in language models?,"[""Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##""]","DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks, with a pass@1 score on AIME 2024 increasing from 15.6% to 71.0%, and further improving to 86.7% with majority voting, matching the performance of OpenAI-o1-0912. However, it also faces challenges such as poor readability and language mixing.",single_hop_specifc_query_synthesizer
What score did DeepSeek-R1 achieve on MATH-500?,"['Distillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-R1 attained an impressive score of 97.3% on MATH-500, performing on par with OpenAI-01-1217 and significantly outperforming other models.",single_hop_specifc_query_synthesizer
What is the performance comparison between DeepSeek-R1 and DeepSeek-V3?,"['Distillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-R1 significantly outperforms DeepSeek-V3 in various benchmarks. For instance, on MMLU, DeepSeek-R1 achieves a score of 90.8%, while DeepSeek-V3 scores lower. Additionally, on the factual benchmark SimpleQA, DeepSeek-R1 also surpasses DeepSeek-V3, demonstrating its superior capability in handling fact-based queries.",single_hop_specifc_query_synthesizer
How does DeepSeek-R1 perform on the LiveCodeBench benchmark?,"['Distillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-R1 achieves a score of 57.2% on the LiveCodeBench benchmark, which significantly outperforms previous open-source models and is comparable to o1-mini.",single_hop_specifc_query_synthesizer
DeepSeek-V3 do better than DeepSeek-R1?,"['Distillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-V3 does not perform as well as DeepSeek-R1. In various benchmarks, DeepSeek-R1 significantly outperforms DeepSeek-V3, achieving higher scores on MMLU, MMLU-Pro, GPQA Diamond, and on the factual benchmark SimpleQA.",single_hop_specifc_query_synthesizer
How does DeepSeek-V3 compare to DeepSeek-R1 in terms of performance on reasoning tasks and benchmarks?,"['Distillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-V3 performs slightly worse than DeepSeek-R1 on various benchmarks. For instance, on MMLU, DeepSeek-R1 achieves a score of 90.8%, while DeepSeek-V3 scores lower. Similarly, on the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, indicating its superior capability in handling fact-based queries. Overall, DeepSeek-R1 demonstrates a competitive edge in educational tasks and reasoning capabilities compared to DeepSeek-V3.",single_hop_specifc_query_synthesizer
What are the results of DeepSeek-R1 on the LiveCodeBench benchmark?,"['Distillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-R1-Distill-Qwen-32B scores 57.2% on LiveCodeBench, which is part of the evaluation results demonstrating its performance on various benchmarks.",single_hop_specifc_query_synthesizer
What is the significance of RL in the training of DeepSeek-R1-Zero?,"['tags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","The significance of RL in the training of DeepSeek-R1-Zero lies in its role as the source of the training signal, which determines the optimization direction. The RL algorithm enables DeepSeek-R1-Zero to enhance its performance over time, as evidenced by a significant increase in the average pass@1 score on the AIME 2024 benchmark, rising from 15.6% to 71.0%. This improvement highlights the efficacy of the RL approach in optimizing the model's reasoning capabilities without the need for supervised fine-tuning data.",single_hop_specifc_query_synthesizer
Can you explain how OpenAI-o1-0912 compares to DeepSeek-R1-Zero in terms of performance on reasoning-related benchmarks?,"['tags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","OpenAI-o1-0912 demonstrates a pass@1 score of 74.4% on the AIME 2024 benchmark, while DeepSeek-R1-Zero shows a pass@1 score of 71.0%. However, when majority voting is applied, DeepSeek-R1-Zero's performance increases to 86.7%, surpassing OpenAI-o1-0912. This indicates that while OpenAI-o1-0912 initially performs better, DeepSeek-R1-Zero has the potential to exceed its performance through specific enhancements.",single_hop_specifc_query_synthesizer
What is the purpose of the Training Template in the context of DeepSeek-R1-Zero?,"['tags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","The Training Template is designed to guide the base model, DeepSeek-R1-Zero, to adhere to specified instructions by producing a reasoning process followed by the final answer. This structural format helps avoid content-specific biases and allows for accurate observation of the model's natural progression during the reinforcement learning process.",single_hop_specifc_query_synthesizer
How does DeepSeek-R1-Zero perform on the AIME 2024 benchmark during its training process?,"['tags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance on the AIME 2024 benchmark throughout the RL training process. The average pass@1 score shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This improvement highlights the efficacy of the RL algorithm in optimizing the model's performance over time.",single_hop_specifc_query_synthesizer
What is the performance comparison between DeepSeek-R1-Zero and OpenAI-o1-0912?,"['tags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","The performance comparison reveals that DeepSeek-R1-Zero achieved a pass@1 score of 71.0% on the AIME 2024 benchmark, while OpenAI-o1-0912 had a higher pass@1 score of 74.4%. Additionally, DeepSeek-R1-Zero's performance can be further enhanced through majority voting, increasing its score to 86.7%, surpassing OpenAI-o1-0912.",single_hop_specifc_query_synthesizer
How does DeepSeek-R1-Zero utilize reward modeling to enhance its reasoning capabilities during training?,"['tags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","DeepSeek-R1-Zero employs a rule-based reward system that consists of two primary types of rewards: accuracy rewards and format rewards. The accuracy reward model assesses whether the response is correct, particularly in scenarios like math problems where deterministic results are expected. This model requires the final answer to be presented in a specified format, allowing for reliable verification of correctness. Additionally, the format reward model mandates that the model's reasoning process be enclosed within `<think>` and `</think>` tags. This structured approach helps avoid biases and ensures that the model's natural progression during the reinforcement learning (RL) process can be accurately observed.",single_hop_specifc_query_synthesizer
Can you explain how DeepSeek-R1-Zero was developed and what makes it significant in the field of AI research?,"['Contributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##']","DeepSeek-R1-Zero was developed by directly applying reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This method allows the model to explore chain-of-thought (CoT) for solving complex problems. It demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Importantly, it is the first open research to validate that reasoning capabilities of large language models (LLMs) can be incentivized purely through RL, without the need for SFT, paving the way for future advancements in this area.",single_hop_specifc_query_synthesizer
How has the research community benefited from the development of DeepSeek-R1-Zero?,"['Contributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##']","The research community has benefited from the development of DeepSeek-R1-Zero as it marks a significant milestone by demonstrating capabilities such as self-verification, reflection, and generating long chain-of-thoughts (CoTs). It is the first open research to validate that reasoning capabilities of large language models (LLMs) can be incentivized purely through reinforcement learning (RL), without the need for supervised fine-tuning (SFT). This breakthrough paves the way for future advancements in the area of reasoning capabilities.",single_hop_specifc_query_synthesizer
What DeepSeek-R1 do?,"['Contributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##']","DeepSeek-R1 is developed through a pipeline that incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities.",single_hop_specifc_query_synthesizer
What role does SFT play in the development of DeepSeek-R1?,"['Contributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##']","In the development of DeepSeek-R1, SFT serves as a preliminary step in the pipeline, which includes two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that provide the seed for the models' reasoning and non-reasoning capabilities.",single_hop_specifc_query_synthesizer
What DeepSeek-R1-Zero do for reasoning in AI?,"['Contributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##']","DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long chain-of-thoughts (CoTs), marking a significant milestone for the research community. It is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through reinforcement learning (RL), without the need for supervised fine-tuning (SFT).",single_hop_specifc_query_synthesizer
What are the key advancements introduced by DeepSeek-R1-Zero in the context of reinforcement learning and reasoning capabilities?,"['Contributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##']","DeepSeek-R1-Zero represents a significant advancement in the application of reinforcement learning (RL) to large language models (LLMs) by demonstrating that reasoning capabilities can be incentivized purely through RL, without the need for supervised fine-tuning (SFT). This model showcases features such as self-verification, reflection, and the ability to generate long chains of thought (CoTs), marking a milestone for the research community. Additionally, the development pipeline for DeepSeek-R1 includes two RL stages focused on improving reasoning patterns and aligning with human preferences, alongside two SFT stages that enhance the model's reasoning and non-reasoning capabilities.",single_hop_specifc_query_synthesizer
What is discussed on Pages 9 and 10?,"[""number of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","Pages 9 and 10 discuss an intriguing phenomenon observed during the training of DeepSeek-R1-Zero, specifically the occurrence of an aha moment. This moment illustrates how the model learns to allocate more thinking time to a problem by reevaluating its initial approach, showcasing its growing reasoning abilities and the unexpected outcomes of reinforcement learning.",single_hop_specifc_query_synthesizer
What notable behavior does DeepSeek-R1-Zero exhibit during its training process?,"[""number of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","DeepSeek-R1-Zero exhibits the remarkable behavior of reflection, where the model revisits and reevaluates its previous steps, as well as the exploration of alternative approaches to problem-solving. These behaviors emerge spontaneously as a result of the model's interaction with the reinforcement learning environment, significantly enhancing its reasoning capabilities.",single_hop_specifc_query_synthesizer
What insights can be drawn from Table 3 regarding the aha moment experienced by DeepSeek-R1-Zero during its training process?,"[""number of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","Table 3 illustrates a significant aha moment for DeepSeek-R1-Zero, occurring in an intermediate version of the model. During this phase, the model learns to allocate more thinking time to problems by reevaluating its initial approaches. This behavior highlights the model's growing reasoning abilities and exemplifies how reinforcement learning can lead to unexpected and sophisticated outcomes. The aha moment serves as a powerful reminder of the potential of reinforcement learning to unlock new levels of intelligence in artificial systems, showcasing the model's ability to autonomously develop advanced problem-solving strategies.",single_hop_specifc_query_synthesizer
What significant observation is made on Page Number 8 regarding the reasoning capabilities of DeepSeek-R1-Zero during its training process?,"[""number of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","On Page Number 8, a significant observation is made regarding the occurrence of an 'aha moment' during the training of DeepSeek-R1-Zero. This moment illustrates how the model learns to allocate more thinking time to a problem by reevaluating its initial approach, showcasing its growing reasoning abilities. This phenomenon highlights the unexpected and sophisticated outcomes that can arise from reinforcement learning, emphasizing the model's autonomous development of advanced problem-solving strategies.",single_hop_specifc_query_synthesizer
How does reinforcement learning contribute to the development of reasoning capabilities in DeepSeek-R1-Zero?,"[""number of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","Reinforcement learning significantly enhances DeepSeek-R1-Zero's reasoning capabilities by allowing the model to autonomously develop advanced problem-solving strategies. As the model interacts with the reinforcement learning environment, it experiences spontaneous developments, such as sophisticated behaviors like reflection and the exploration of alternative approaches to problem-solving. This self-evolution enables DeepSeek-R1-Zero to tackle increasingly complex reasoning tasks with greater efficiency and accuracy.",single_hop_specifc_query_synthesizer
What does Table 3 illustrate about DeepSeek-R1-Zero's reasoning capabilities?,"[""number of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","Table 3 illustrates an intriguing aha moment of an intermediate version of DeepSeek-R1-Zero, where the model learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior showcases the model's growing reasoning abilities and exemplifies how reinforcement learning can lead to unexpected and sophisticated outcomes.",single_hop_specifc_query_synthesizer
How does reinforcement learning (RL) enhance reasoning capabilities in DeepSeek-R1?,"['DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","Reinforcement learning (RL) enhances reasoning capabilities in DeepSeek-R1 by applying a large-scale training process that focuses on reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning. After fine-tuning the model on cold start data, RL training is employed to improve the model's performance, particularly by introducing a language consistency reward to mitigate language mixing issues. The final reward combines the accuracy of reasoning tasks with the language consistency reward, guiding the model until it achieves convergence on reasoning tasks.",single_hop_specifc_query_synthesizer
What are the key components of Reinforcement Learning as described in the context?,"['DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The key components of Reinforcement Learning (RL) as described in the context include a training pipeline that consists of several stages: 1) Cold Start, where a small amount of high-quality data is used to fine-tune the model; 2) Reasoning-oriented Reinforcement Learning, which enhances the model's reasoning capabilities through a large-scale training process; 3) Rejection Sampling and Supervised Fine-Tuning, where reasoning and non-reasoning data are curated to improve the model's performance; and 4) Reinforcement Learning for all Scenarios, aimed at aligning the model with human preferences by improving helpfulness and harmlessness while refining reasoning capabilities.",single_hop_specifc_query_synthesizer
How does the incorporation of Chains of Thought (CoT) data enhance the reasoning capabilities of the DeepSeek-R1 model during its training process?,"['DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The incorporation of Chains of Thought (CoT) data enhances the reasoning capabilities of the DeepSeek-R1 model by providing a structured approach to fine-tuning the model as the initial reinforcement learning actor. This is achieved by collecting a small amount of long CoT data to prevent instability during the early cold start phase of reinforcement learning training. The CoT data is designed to be readable and includes a summary at the end of each response, which helps in filtering out non-reader-friendly outputs. By carefully designing the cold-start data with human priors, the model demonstrates improved performance compared to its predecessor, DeepSeek-R1-Zero. Additionally, the training process focuses on reasoning-intensive tasks and incorporates a language consistency reward to mitigate issues such as language mixing, further enhancing the model's overall reasoning capabilities.",single_hop_specifc_query_synthesizer
What are the limitations of DeepSeek-R1-Zero compared to DeepSeek-R1?,"['DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, a readable pattern is designed that includes a summary at the end of each response and filters out responses that are not reader-friendly.",single_hop_specifc_query_synthesizer
What does CoT stand for in the context of DeepSeek-R1?,"['DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","CoT stands for Chains of Thought, which are designed to produce clear and coherent reasoning processes in the DeepSeek-R1 model.",single_hop_specifc_query_synthesizer
What is CoT in the context of DeepSeek-R1?,"['DeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","CoT stands for Chains of Thought, which are designed to produce clear and coherent reasoning processes in the DeepSeek-R1 model. The model aims to generate readable outputs that include a reasoning process and a summary, enhancing its reasoning capabilities.",single_hop_specifc_query_synthesizer
What are the evaluation results reported for AIME 2024?,"['Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.",single_hop_specifc_query_synthesizer
What is the significance of the HumanEval-Mul dataset in evaluating AI models?,"['Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","The HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash) and is used to evaluate model performance on LiveCodeBench using CoT format. This dataset is crucial for assessing the reasoning capabilities of AI models in coding tasks.",single_hop_specifc_query_synthesizer
What is the significance of AIME 2024 in the context of model evaluation?,"['Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","AIME 2024 is significant as it serves as one of the benchmarks for evaluating distilled models, alongside other tasks like MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. The results reported for AIME 2024 help demonstrate the effectiveness of the distillation technique applied to enhance reasoning capabilities in smaller models.",single_hop_specifc_query_synthesizer
How is MMLU utilized in evaluating models for reasoning capabilities?,"['Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","MMLU is used as a benchmark to evaluate models on their reasoning capabilities. The models are assessed on MMLU along with other benchmarks like MMLU-Redux and MMLU-Pro, using prompts from the simple-evals framework. The evaluation setup includes a zero-shot setting for MMLU-Redux and slight modifications for MMLU-Pro to adapt to a zero-shot context, ensuring a comprehensive assessment of the models' performance.",single_hop_specifc_query_synthesizer
What are the evaluation results reported for AIME 2024?,"['Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.",single_hop_specifc_query_synthesizer
"Can you explain the significance of MMLU in evaluating AI models, particularly in the context of reasoning capabilities?","['Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","MMLU is a benchmark used to evaluate models on their reasoning capabilities. In the context provided, models are assessed on MMLU along with other benchmarks like MMLU-Redux and MMLU-Pro. The evaluation setup includes various configurations to ensure accurate performance estimates, such as using prompts from the simple-evals framework and adopting a zero-shot setting for MMLU-Redux. The results indicate that models like DeepSeek-R1 have shown significant improvements in reasoning abilities, as evidenced by their high pass rates on MMLU compared to other models.",single_hop_specifc_query_synthesizer
Coud you please explain the significance of ArenaHard in the context of AI model evaluations and its performance metrics?,"['V3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","ArenaHard is highlighted as a benchmark in the evaluation of AI models, particularly in the context of DeepSeek-R1, which demonstrates impressive performance on this task. The context indicates that DeepSeek-R1 shows significant strengths in writing tasks and open-domain question answering on ArenaHard, outperforming DeepSeek-V3. This suggests that ArenaHard serves as an important metric for assessing the reasoning capabilities and generalization benefits of AI models, particularly those trained with large-scale reinforcement learning.",single_hop_specifc_query_synthesizer
What is ArenaHard in the context of AI benchmarks?,"['V3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","ArenaHard is mentioned as a benchmark in the context of evaluating AI models, specifically highlighting its performance metrics for DeepSeek-R1, which shows significant strengths in writing tasks and open-domain question answering.",single_hop_specifc_query_synthesizer
Can you explain what MMLU is and how it relates to the performance of AI models like DeepSeek-R1?,"['V3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","MMLU is an education-oriented knowledge benchmark that evaluates the performance of AI models on various tasks. In the context provided, DeepSeek-R1 demonstrates superior performance on MMLU compared to DeepSeek-V3, particularly due to enhanced accuracy in STEM-related questions achieved through large-scale reinforcement learning.",single_hop_specifc_query_synthesizer
Can you explain how AlpacaEval2.0 contributes to the performance of DeepSeek-R1 in reasoning tasks?,"['V3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","AlpacaEval2.0 indicates that DeepSeek-R1 demonstrates impressive performance, particularly in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 highlights the generalization benefits of large-scale reinforcement learning, which not only boosts reasoning capabilities but also improves performance across diverse domains.",single_hop_specifc_query_synthesizer
Can you tell me how GPT-4o 0513 performs in comparison to other models on various benchmarks?,"['V3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","GPT-4o 0513 shows a performance of 85.0 on several benchmarks, including the ArenaHard and Chinese benchmarks like CLUEWSC and C-Eval. In coding benchmarks, it scores 50.8 on LiveCodeBench and Aider-Polyglot. For math tasks, it achieves 16.0 on AIME 2024 and 23.3 on MATH 500. However, it performs worse than models like Claude-3.5-Sonnet-1022 on some tasks, indicating a competitive but not leading position in the overall evaluation.",single_hop_specifc_query_synthesizer
"How does MMLU performance compare across different models, particularly in relation to DeepSeek-R1?","['V3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","In the context of education-oriented knowledge benchmarks such as MMLU, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities.",single_hop_specifc_query_synthesizer
How does the performance of OpenAI-01-1217 compare to that of DeepSeek-R1 in terms of reasoning capabilities across various tasks?,"['of Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.']","OpenAI-01-1217 achieves performance comparable to DeepSeek-R1 across a range of tasks. While DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative reinforcement learning fine-tuning, both models demonstrate strong reasoning abilities. The results indicate that distilling the reasoning capability to smaller dense models can yield promising outcomes, with DeepSeek-R1-Distill-Qwen-1.5B outperforming other models like GPT-40 and Claude-3.5-Sonnet on specific benchmarks.",single_hop_specifc_query_synthesizer
What is the significance of AlphaGo in the context of Monte Carlo Tree Search?,"['of Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.']",AlphaGo is significant in the context of Monte Carlo Tree Search (MCTS) as it inspired the exploration of using MCTS to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to systematically explore the solution space.,single_hop_specifc_query_synthesizer
What are the performance metrics for DeepSeek-R1-Zero-Qwen-32B on reasoning-related benchmarks?,"['of Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.']","DeepSeek-R1-Zero-Qwen-32B achieved a pass@1 score of 47.0 on the AIME 2024 benchmark, 60.0 on the cons@64 metric, 91.6 on the MATH-500 benchmark, 55.0 on the GPQA Diamond benchmark, and 40.2 on the LiveCodeBench.",single_hop_specifc_query_synthesizer
"What performance metrics does the QwQ-32B-Preview model achieve on various reasoning-related benchmarks, and how does it compare to other models?","['of Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.']","The QwQ-32B-Preview model achieves the following performance metrics on reasoning-related benchmarks: 50.0 pass@1 and 60.0 cons@64 on AIME 2024, 90.6 pass@1 on MATH-500, 54.5 pass@1 on GPQA Diamond, and 41.9 pass@1 on LiveCodeBench. In comparison, the DeepSeek-R1-Zero-Qwen-32B model scores 47.0 pass@1 and 60.0 cons@64 on AIME 2024, 91.6 pass@1 on MATH-500, 55.0 pass@1 on GPQA Diamond, and 40.2 pass@1 on LiveCodeBench. The DeepSeek-R1-Distill-Qwen-32B model outperforms both, with scores of 72.6 pass@1 and 83.3 cons@64 on AIME 2024, 94.3 pass@1 on MATH-500, 62.1 pass@1 on GPQA Diamond, and 57.2 pass@1 on LiveCodeBench.",single_hop_specifc_query_synthesizer
What does LiveCodeBench measure in the context of AI models?,"['of Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.']","LiveCodeBench is one of the benchmarks used to evaluate the performance of AI models, specifically in the context of reasoning-related tasks, as indicated in the comparison table of distilled and RL models.",single_hop_specifc_query_synthesizer
What is the significance of LiveCodeBench in the context of reasoning-related benchmarks?,"['of Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.']","LiveCodeBench is included in a comparison of distilled and RL models on reasoning-related benchmarks, where it is evaluated alongside other models such as QwQ-32B-Preview and DeepSeek-R1-Distill-Qwen-32B. The performance metrics for LiveCodeBench indicate its effectiveness in achieving a pass rate of 41.9% in the benchmark evaluations.",single_hop_specifc_query_synthesizer
What are the evaluation results of DeepSeek-R1 on MMLU and how do they compare to DeepSeek-V3?,"['<1-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek', '<2-hop>\n\nV3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","DeepSeek-R1 demonstrates superior performance on the MMLU benchmark compared to DeepSeek-V3. Specifically, DeepSeek-R1 achieved a pass@1 score of 92.9 on MMLU, while DeepSeek-V3 scored 88.5. This improvement is attributed to enhanced accuracy in STEM-related questions, showcasing the effectiveness of the model's reasoning capabilities.",multi_hop_specific_query_synthesizer
"What are the evaluation results of DeepSeek-R1 on MMLU and MMLU-Pro benchmarks, and how do they compare to DeepSeek-V3?","['<1-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek', '<2-hop>\n\nV3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","DeepSeek-R1 demonstrates superior performance on the MMLU and MMLU-Pro benchmarks compared to DeepSeek-V3. Specifically, for MMLU, DeepSeek-R1 achieved a Pass@1 score of 92.9, while DeepSeek-V3 scored 88.5. Similarly, on the MMLU-Pro benchmark, DeepSeek-R1 also outperformed DeepSeek-V3 with a Pass@1 score of 92.9 compared to 88.5 for DeepSeek-V3. This improvement is attributed to enhanced accuracy in STEM-related questions, showcasing the effectiveness of the model's training and evaluation methods.",multi_hop_specific_query_synthesizer
"What are the performance metrics of DeepSeek-R1 on MMLU and MMLU-Pro benchmarks, and how do they compare to DeepSeek-V3?","['<1-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek', '<2-hop>\n\nV3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","DeepSeek-R1 demonstrates superior performance on the MMLU and MMLU-Pro benchmarks compared to DeepSeek-V3. Specifically, for MMLU, DeepSeek-R1 achieved a Pass@1 score of 92.9, while DeepSeek-V3 scored 88.5. Similarly, on the MMLU-Pro benchmark, DeepSeek-R1 also scored 92.9, outperforming DeepSeek-V3, which had a score of 88.5. This improvement is attributed to enhanced accuracy in STEM-related questions, showcasing the effectiveness of the model's reasoning capabilities.",multi_hop_specific_query_synthesizer
"What are the comparative performance results of DeepSeek-R1-Zero and OpenAI-01-1217 on reasoning tasks, and how does the self-evolution process of DeepSeek-R1-Zero contribute to its performance improvements?","['<1-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","DeepSeek-R1-Zero demonstrates competitive performance in reasoning tasks, achieving a score of 79.8% Pass@1 on AIME 2024, which slightly surpasses OpenAI-01-1217. On MATH-500, DeepSeek-R1-Zero attains an impressive score of 97.3%, performing on par with OpenAI-01-1217. Additionally, DeepSeek-R1-Zero shows a significant improvement in its performance trajectory throughout the reinforcement learning (RL) training process, with its average pass@1 score on AIME 2024 increasing from 15.6% to 71.0%. This self-evolution process allows DeepSeek-R1-Zero to enhance its reasoning capabilities autonomously, showcasing its ability to learn and generalize effectively through RL alone, without the need for supervised fine-tuning data.",multi_hop_specific_query_synthesizer
"How does DeepSeek-R1 compare to OpenAI-01-1217 in reasoning tasks, and what improvements were observed in DeepSeek-R1-Zero's performance over time?","['<1-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","DeepSeek-R1 demonstrates competitive performance in reasoning tasks, achieving a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. In comparison, DeepSeek-R1-Zero shows significant improvement over time, with its average pass@1 score on AIME 2024 increasing from 15.6% to 71.0% during the reinforcement learning training process, reaching performance levels comparable to OpenAI-01-0912. This highlights the efficacy of the reinforcement learning approach in enhancing reasoning capabilities.",multi_hop_specific_query_synthesizer
"What are the comparative performance results of DeepSeek-R1-Zero and OpenAI-01-0912 on the AIME 2024 benchmark, and how does the self-evolution process of DeepSeek-R1-Zero contribute to its reasoning capabilities?","['<1-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","On the AIME 2024 benchmark, DeepSeek-R1-Zero achieves a pass@1 score of 71.0%, which is comparable to OpenAI-01-0912. The self-evolution process of DeepSeek-R1-Zero, driven by reinforcement learning (RL) without the need for supervised fine-tuning data, allows it to autonomously improve its reasoning capabilities over time. This process is evidenced by a consistent increase in performance, as shown in training evaluations, where DeepSeek-R1-Zero's accuracy improves significantly, highlighting its ability to learn and generalize effectively in reasoning tasks.",multi_hop_specific_query_synthesizer
How does DeepSeek-R1 improve reasoning capabilities compared to DeepSeek-R1-Zero using cold start data?,"[""<1-hop>\n\nIntroduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##"", '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","DeepSeek-R1 improves reasoning capabilities compared to DeepSeek-R1-Zero by incorporating a small amount of high-quality cold start data during its training process. This cold start data is collected to fine-tune the DeepSeek-V3-Base model, which helps stabilize the early phases of reinforcement learning (RL) training. Unlike DeepSeek-R1-Zero, which faced challenges such as poor readability and language mixing, DeepSeek-R1 is designed to produce clearer and more coherent Chains of Thought (CoT). The training pipeline for DeepSeek-R1 includes a focus on generating readable outputs and filtering out responses that are not user-friendly, ultimately leading to better performance in reasoning tasks.",multi_hop_specific_query_synthesizer
"How does the incorporation of cold-start data in the training of DeepSeek-R1 improve its reasoning capabilities compared to DeepSeek-R1-Zero, particularly in terms of user-friendly output and performance on reasoning benchmarks?","[""<1-hop>\n\nIntroduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##"", '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The incorporation of cold-start data in the training of DeepSeek-R1 significantly improves its reasoning capabilities compared to DeepSeek-R1-Zero by providing a stable initial phase for reinforcement learning (RL). This cold-start phase involves collecting a small amount of long Chain of Thought (CoT) data to fine-tune the model, which helps prevent instability during early RL training. The cold-start data is designed to be readable and coherent, addressing the key limitation of DeepSeek-R1-Zero, which often produced outputs that were not suitable for reading, including language mixing and lack of formatting. As a result, DeepSeek-R1 not only enhances the clarity of its responses but also achieves better performance on reasoning benchmarks, with the pass@1 score on AIME 2024 increasing significantly. This structured approach to training, which includes a focus on user-friendly output, allows DeepSeek-R1 to match the performance of OpenAI's models while maintaining strong reasoning capabilities.",multi_hop_specific_query_synthesizer
How does the incorporation of cold-start data in DeepSeek-R1 improve the reasoning capabilities of the DeepSeek-V3-Base model compared to DeepSeek-R1-Zero?,"[""<1-hop>\n\nIntroduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##"", '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The incorporation of cold-start data in DeepSeek-R1 improves the reasoning capabilities of the DeepSeek-V3-Base model by providing a stable initial phase for reinforcement learning (RL) training. Unlike DeepSeek-R1-Zero, which faced instability during the early RL training phase, DeepSeek-R1 uses a small amount of long Chain of Thought (CoT) data to fine-tune the model as the initial RL actor. This approach allows for better performance by ensuring that the model starts with high-quality data, which enhances its reasoning capabilities and reduces issues such as language mixing and poor readability that were prevalent in DeepSeek-R1-Zero.",multi_hop_specific_query_synthesizer
"What are the performance improvements of DeepSeek-R1 compared to DeepSeek-V3-Base in reasoning tasks, and how does it relate to the advancements in the DeepSeek model series?","[""<1-hop>\n\nIntroduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##"", '<2-hop>\n\nV3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","DeepSeek-R1 demonstrates significant performance improvements over DeepSeek-V3-Base in reasoning tasks. For instance, the pass@1 score on AIME 2024 increases from 15.6% for DeepSeek-V3 to 71.0% for DeepSeek-R1, with majority voting further enhancing the score to 86.7%, matching the performance of OpenAI-o1-0912. Additionally, DeepSeek-R1 outperforms DeepSeek-V3 in various benchmarks, including math and coding tasks, showcasing its enhanced reasoning capabilities. This advancement is attributed to the incorporation of a multi-stage training pipeline and the use of reinforcement learning, which allows DeepSeek-R1 to evolve its reasoning capabilities without relying on supervised data, thus marking a significant step forward in the DeepSeek model series.",multi_hop_specific_query_synthesizer
What advancements in reasoning capabilities were achieved with the DeepSeek-V3-Base model and how does DeepSeek-R1 compare to OpenAI's o1 series models in terms of performance on reasoning benchmarks?,"[""<1-hop>\n\nIntroduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##"", '<2-hop>\n\nV3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","The DeepSeek-V3-Base model served as the foundation for enhancing reasoning capabilities through a pure reinforcement learning (RL) process. This approach led to the emergence of DeepSeek-R1-Zero, which demonstrated significant improvements on reasoning benchmarks, achieving a pass@1 score of 71.0% on AIME 2024, and further improving to 86.7% with majority voting. In comparison, DeepSeek-R1 achieved a pass@1 score of 79.0%, outperforming OpenAI's o1-1217, which scored 78.0%. This indicates that DeepSeek-R1 not only matches but also exceeds the performance of OpenAI's models in certain reasoning tasks, showcasing the effectiveness of the advancements made with the DeepSeek series.",multi_hop_specific_query_synthesizer
"What advancements in reasoning capabilities have been achieved by OpenAI's models compared to DeepSeek-R1, and how do these models perform on various benchmarks?","[""<1-hop>\n\nIntroduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##"", '<2-hop>\n\nV3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","OpenAI's models, particularly the o1 series, have made significant advancements in reasoning capabilities, especially through the introduction of inference-time scaling that enhances the Chain-of-Thought reasoning process. This has led to notable improvements in various reasoning tasks, including mathematics, coding, and scientific reasoning. In comparison, DeepSeek-R1 has demonstrated superior performance on reasoning benchmarks, achieving a pass@1 score of 71.0% on AIME 2024, which is a substantial increase from its predecessor, DeepSeek-V3. Furthermore, DeepSeek-R1 outperforms OpenAI's o1-mini and shows competitive results against OpenAI o1-1217, with scores of 79.0% and 91.0% respectively on different benchmarks. The performance of these models highlights the effectiveness of large-scale reinforcement learning and the distillation process in enhancing reasoning capabilities across diverse domains.",multi_hop_specific_query_synthesizer
"What are the performance results of DeepSeek-R1-Distill-Qwen-32B compared to other models on reasoning-related benchmarks, and how does this relate to the distillation process discussed?","['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-R1-Distill-Qwen-32B achieves impressive scores of 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench, significantly outperforming previous open-source models. This performance is a result of the distillation process, which demonstrates that reasoning patterns from larger models can be effectively distilled into smaller models, leading to better performance compared to those discovered through reinforcement learning on smaller models. The results indicate that distilling more powerful models into smaller ones yields excellent results, as seen with DeepSeek-R1-Distill-Qwen-32B's performance surpassing that of QwQ-32B-Preview.",multi_hop_specific_query_synthesizer
"How does the performance of DeepSeek-R1 compare to OpenAI-01-1217 across various reasoning-related benchmarks, and what implications does this have for the development of smaller models through distillation?","['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. The results indicate that distilling the reasoning patterns of larger models like DeepSeek-R1 into smaller models can yield better performance, as demonstrated by DeepSeek-R1-Distill-Qwen-32B, which scores 72.6% on AIME 2024 and 94.3% on MATH-500. This suggests that smaller models can be powerful too, benefiting the research community by providing effective alternatives to larger models while maintaining competitive performance.",multi_hop_specific_query_synthesizer
What are the performance comparisons between QwQ-32B-Preview and the distilled models like DeepSeek-R1-Distill-Qwen-32B on reasoning-related benchmarks?,"['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","The performance comparisons indicate that QwQ-32B-Preview achieves a score of 50.0% on AIME 2024, while the distilled model DeepSeek-R1-Distill-Qwen-32B significantly outperforms it with a score of 72.6% on the same benchmark. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 94.3% on MATH-500 and 57.2% on LiveCodeBench, showcasing its superior performance compared to QwQ-32B-Preview.",multi_hop_specific_query_synthesizer
How does the cold start data improve the reasoning capabilities in RL for DeepSeek-R1 compared to DeepSeek-R1-Zero?,"['<1-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","The cold start data improves the reasoning capabilities in RL for DeepSeek-R1 by providing a small amount of high-quality data to fine-tune the model as the initial RL actor. This approach prevents the early unstable cold start phase of RL training seen in DeepSeek-R1-Zero. By collecting thousands of long chain-of-thought (CoT) data and designing a readable pattern for the output, DeepSeek-R1 demonstrates better performance against DeepSeek-R1-Zero, particularly in reasoning-intensive tasks. The iterative training process, enhanced by the cold start data, allows for a more stable and effective learning experience.",multi_hop_specific_query_synthesizer
"What are the key improvements in reasoning capabilities of DeepSeek-R1 compared to DeepSeek-R1-Zero, particularly in the context of reinforcement learning (RL) and the use of cold start data?","['<1-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","DeepSeek-R1 demonstrates significant improvements in reasoning capabilities compared to DeepSeek-R1-Zero by incorporating a cold start phase that utilizes a small amount of high-quality data to fine-tune the model as the initial RL actor. This approach helps prevent the early unstable cold start phase of RL training seen in DeepSeek-R1-Zero. Additionally, DeepSeek-R1 focuses on enhancing reasoning capabilities through a structured pipeline that includes collecting long Chains of Thought (CoT) data, which improves readability and coherence in responses. The introduction of a language consistency reward during RL training further aligns the model's outputs with human preferences, making them more readable. Overall, these enhancements lead to better performance in reasoning-intensive tasks, as evidenced by the model's ability to achieve higher accuracy and a more stable learning trajectory.",multi_hop_specific_query_synthesizer
"How does the incorporation of cold start data in DeepSeek-R1 improve its reasoning capabilities compared to DeepSeek-R1-Zero, particularly in the context of reinforcement learning?","['<1-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","The incorporation of cold start data in DeepSeek-R1 significantly improves its reasoning capabilities compared to DeepSeek-R1-Zero by providing a small amount of high-quality data to fine-tune the model as the initial RL actor. This approach helps prevent the early unstable cold start phase of RL training that was a limitation in DeepSeek-R1-Zero. By collecting thousands of long chain-of-thought (CoT) data through various methods, such as few-shot prompting and human annotation, DeepSeek-R1 is able to produce clearer and more coherent reasoning outputs. Additionally, the structured format of the cold-start data enhances readability, which is crucial for user engagement. The iterative training process employed in DeepSeek-R1 further optimizes the model's performance in reasoning-intensive tasks, leading to better overall results in comparison to its predecessor.",multi_hop_specific_query_synthesizer
"What are the performance comparisons between DeepSeek-R1-Zero and OpenAI-01-0912 on reasoning-related benchmarks, and how does the distillation process impact the results?","['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","The performance comparisons between DeepSeek-R1-Zero and OpenAI-01-0912 on reasoning-related benchmarks reveal that DeepSeek-R1-Zero achieves a pass@1 score of 71.0% on the AIME 2024 benchmark, which is comparable to OpenAI-01-0912's score of 74.4%. Additionally, the distillation process significantly impacts the results, as DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks, achieving 28.9% on AIME and 83.9% on MATH. This indicates that distilling more powerful models into smaller ones yields excellent results, enhancing the reasoning capabilities of the distilled models.",multi_hop_specific_query_synthesizer
"What are the performance comparisons between DeepSeek-R1-Zero and OpenAI-01-0912 on reasoning-related benchmarks, and how does the distillation process impact the performance of smaller models?","['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","DeepSeek-R1-Zero demonstrates strong performance across various reasoning-related benchmarks, achieving a pass@1 score of 71.0% on the AIME 2024 benchmark, which is comparable to OpenAI-01-0912's score of 74.4%. The distillation process significantly impacts the performance of smaller models, as evidenced by DeepSeek-R1-Distill-Qwen-32B outperforming other instruction-tuned models based on the same underlying checkpoints. This distillation allows smaller models to achieve impressive results, such as DeepSeek-R1-Distill-0wen-1.5B outperforming GPT-40 and Claude-3.5-Sonnet on math benchmarks with scores of 28.9% on AIME and 83.9% on MATH.",multi_hop_specific_query_synthesizer
"What are the performance comparisons between DeepSeek-R1-Zero and OpenAI-01-1217 in reasoning tasks, and how does the distillation process contribute to these results?","['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","DeepSeek-R1-Zero achieves performance comparable to OpenAI-01-1217 across various reasoning tasks. The distillation process plays a significant role in this achievement, as DeepSeek-R1 is used as the teacher model to generate training samples for smaller dense models. The results indicate that DeepSeek-R1-Distill-Qwen-1.5B outperforms other models like GPT-40 and Claude-3.5-Sonnet on math benchmarks, achieving 28.9% on AIME and 83.9% on MATH. This demonstrates that distilling more powerful models into smaller ones yields excellent results, enhancing the reasoning capabilities of the distilled models.",multi_hop_specific_query_synthesizer
What are the performance metrics of the DeepSeek-R1-Distill-Qwen-32B model on the MATH-500 benchmark compared to other models?,"['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The DeepSeek-R1-Distill-Qwen-32B model achieved a pass@1 score of 94.3 on the MATH-500 benchmark, which is significantly higher than the scores of other models listed. For instance, QwQ-32B-Preview scored 90.6, and DeepSeek-R1-Zero-Qwen-32B scored 91.6. This indicates that the distillation process from more powerful models into smaller ones yields excellent results, particularly in reasoning-related benchmarks.",multi_hop_specific_query_synthesizer
"What are the performance results of the DeepSeek-R1-Distill-Qwen-32B model on the MATH-500 benchmark compared to other models, and how does this relate to the overall advancements in reasoning capabilities?","['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The DeepSeek-R1-Distill-Qwen-32B model achieved a performance of 94.3% on the MATH-500 benchmark, which is significantly higher than the performance of other models such as QwQ-32B-Preview, which scored 90.6%, and DeepSeek-R1-Zero-Qwen-32B, which scored lower. This indicates that distilling more powerful models into smaller ones, like DeepSeek-R1-Distill-Qwen-32B, yields excellent results in reasoning tasks. The advancements in reasoning capabilities are further supported by the overall performance improvements seen in various benchmarks, demonstrating the effectiveness of the distillation process in enhancing model reasoning abilities.",multi_hop_specific_query_synthesizer
"What are the performance metrics of the DeepSeek-R1 models on the MATH-500 benchmark, and how does the cold start approach impact their reasoning capabilities?","['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The performance metrics of the DeepSeek-R1 models on the MATH-500 benchmark indicate that DeepSeek-R1-Distill-Qwen-32B achieves a pass rate of 94.3%, significantly outperforming other models. The cold start approach, which involves fine-tuning the model with a small amount of high-quality data, enhances the reasoning capabilities of DeepSeek-R1 by providing a stable initial phase for reinforcement learning, leading to improved performance in reasoning-intensive tasks.",multi_hop_specific_query_synthesizer
What are the key findings regarding the performance of distilled models on AIME 2024 and how do they compare to larger models like DeepSeek-R1?,"['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","The key findings regarding the performance of distilled models on AIME 2024 indicate that the distillation method significantly enhances the reasoning abilities of smaller models. For instance, the distilled model DeepSeek-R1-Distill-Qwen-1.5B outperformed larger models like GPT-40 and Claude-3.5-Sonnet on math benchmarks, achieving 28.9% on AIME and 83.9% on MATH. This suggests that while larger models like DeepSeek-R1 leverage cold-start data and iterative reinforcement learning for strong performance, effective distillation can empower smaller models to achieve competitive results on reasoning-related benchmarks.",multi_hop_specific_query_synthesizer
What are the performance results of the DeepSeek-R1 model on the AIME 2024 benchmark compared to other models?,"['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","The performance results of the DeepSeek-R1 model on the AIME 2024 benchmark indicate that it significantly outperforms other models. Specifically, DeepSeek-R1 achieved a pass@1 score of 92.9, which is higher than the scores of Claude-3.5-Sonnet-1022 at 88.7, GPT-4o 0513 at 88.8, and even the OpenAI o1-1217 at 91.8. This demonstrates the effectiveness of the DeepSeek-R1 model in reasoning tasks as evaluated on the AIME 2024 benchmark.",multi_hop_specific_query_synthesizer
What are the key findings regarding the performance of distilled models on reasoning benchmarks like AIME 2024 and how do they compare to larger models like DeepSeek-R1?,"['<1-hop>\n\nof Distilled and RL Models on Reasoning-Related Benchmarks <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCodeBench</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.6</td> <td>54.5</td> <td>41.9</td> </tr> <tr> <td>DeepSeek-R1-Zero-Qwen-32B</td> <td>47.0</td> <td>60.0</td> <td>91.6</td> <td>55.0</td> <td>40.2</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>72.6</td> <td>83.3</td> <td>94.3</td> <td>62.1</td> <td>57.2</td> </tr> </table> **Table 6**: Comparison of distilled and RL Models on Reasoning-Related Benchmarks. ## Text RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning. ## Unsuccessful Attempts In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models. ## Process Reward Model (PRM) PRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks (Lightman et al., 2023; Useato et al., 2022; Wang et al., 2023). However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward hacking (Gao et al., 2022), and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search (Snell et al., 2024), its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. ## Monte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Silver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process. However, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an... ## Page Number 15 ## Conclusion, Limitations, and Future Work In this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves performance comparable to OpenAI-01-1217 on a range of tasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-0wen-1.5B outperforms GPT-40 and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints. In the future, we plan to invest in research across the following directions for DeepSeek-R1. - **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields. - **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates. - **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly state the problem and specify the output format using a zero-shot setting for optimal results. - **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the system, DeepSeek-R1 has not been tested extensively in software engineering tasks. We observe DeepSeek-R1 has not been able to achieve the same performance as DeepSeek-V3 on software engineering benchmarks. Future versions will address this by enhancing rejection sampling and software engineering data to incorporate dependency evaluation within the RL process to enhance efficiency.', '<2-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","The key findings regarding the performance of distilled models on reasoning benchmarks such as AIME 2024 indicate that the distillation method significantly enhances the reasoning abilities of smaller models. For instance, the distilled model DeepSeek-R1-Distill-Qwen-32B outperforms other instruction-tuned models based on the same underlying checkpoints, achieving impressive results on AIME 2024. In comparison, the larger model DeepSeek-R1, which leverages cold-start data alongside iterative reinforcement learning fine-tuning, achieves performance comparable to OpenAI-01-1217 across various tasks. This suggests that while distilled models can be highly effective, larger models may still hold an advantage in certain complex reasoning tasks.",multi_hop_specific_query_synthesizer
"In what ways does the incorporation of cold start data in DeepSeek-R1 enhance its reasoning capabilities compared to DeepSeek-R1-Zero, particularly in the context of reinforcement learning?","['<1-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', ""<2-hop>\n\nnumber of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","The incorporation of cold start data in DeepSeek-R1 enhances its reasoning capabilities compared to DeepSeek-R1-Zero by providing a small amount of high-quality data to fine-tune the model as the initial RL actor. This approach prevents the early unstable cold start phase of RL training that was present in DeepSeek-R1-Zero. By collecting thousands of long Chains of Thought (CoT) data, DeepSeek-R1 is able to produce clearer and more coherent responses, which improves readability and overall performance. Additionally, the cold start data is designed to filter out responses that are not reader-friendly, thus ensuring that the model's outputs are more accessible. This careful design leads to better performance in reasoning tasks, as the model can leverage the structured data to enhance its reasoning capabilities, particularly in complex tasks such as coding, mathematics, and logic reasoning.",multi_hop_specific_query_synthesizer
How does the cold start data in DeepSeek-R1 enhance reinforcement learning for reasoning tasks compared to DeepSeek-R1-Zero?,"['<1-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', ""<2-hop>\n\nnumber of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","The cold start data in DeepSeek-R1 enhances reinforcement learning for reasoning tasks by providing a small amount of high-quality data to fine-tune the model as the initial RL actor. This approach prevents the early unstable cold start phase of RL training seen in DeepSeek-R1-Zero. By collecting thousands of cold-start data, which includes long Chains of Thought (CoT) and ensuring readability, DeepSeek-R1 demonstrates better performance in reasoning tasks. Additionally, the iterative training process allows the model to develop sophisticated reasoning capabilities, addressing limitations such as language mixing and poor readability that were present in DeepSeek-R1-Zero.",multi_hop_specific_query_synthesizer
"What are the key improvements in reasoning capabilities observed in DeepSeek-R1 compared to DeepSeek-R1-Zero, particularly in relation to reinforcement learning and the cold start approach?","['<1-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', ""<2-hop>\n\nnumber of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","DeepSeek-R1 shows significant improvements in reasoning capabilities compared to DeepSeek-R1-Zero, particularly through its use of a cold start approach. By incorporating a small amount of high-quality data as a cold start, DeepSeek-R1 fine-tunes the model to prevent instability during the early phases of reinforcement learning (RL) training. This method allows the model to produce clearer and more coherent Chains of Thought (CoT) and enhances its general capabilities. Additionally, the iterative training process in DeepSeek-R1 leads to better performance in reasoning-intensive tasks, as it mitigates issues like language mixing and improves readability. The introduction of a language consistency reward during RL training further aligns the model's outputs with human preferences, making the reasoning process more accessible and effective.",multi_hop_specific_query_synthesizer
"What improvements were observed in reasoning capabilities after fine-tuning DeepSeek-V3-Base with cold start data, and how does this compare to the performance of DeepSeek-R1?","['<1-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', '<2-hop>\n\nV3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","After fine-tuning DeepSeek-V3-Base with cold start data, significant improvements in reasoning capabilities were observed, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning. The training process included a focus on enhancing the model's performance by incorporating a language consistency reward to mitigate issues like language mixing. In comparison, DeepSeek-R1 demonstrated superior performance across various benchmarks, particularly in STEM-related questions, due to its large-scale reinforcement learning training. This led to DeepSeek-R1 outperforming DeepSeek-V3 in handling fact-based queries and achieving better results in document analysis tasks.",multi_hop_specific_query_synthesizer
How does DeepSeek-V3-Base compare to DeepSeek-R1 in terms of reasoning capabilities and performance on benchmarks?,"['<1-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', '<2-hop>\n\nV3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3-Base, particularly in reasoning-related benchmarks. This improvement is attributed to enhanced accuracy in STEM-related questions achieved through large-scale reinforcement learning. DeepSeek-R1 excels on various benchmarks, including education-oriented knowledge assessments like MMLU and GPQA Diamond, showcasing its strong document analysis capabilities. In contrast, DeepSeek-V3 shows limitations, particularly in handling fact-based queries, where DeepSeek-R1 outperforms it. Overall, the iterative training and design improvements in DeepSeek-R1 contribute to its enhanced reasoning capabilities over DeepSeek-V3-Base.",multi_hop_specific_query_synthesizer
"How does the performance of DeepSeek-V3-Base compare to DeepSeek-R1 in reasoning tasks, and what improvements were made in the training process to enhance reasoning capabilities?","['<1-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.', '<2-hop>\n\nV3: 71.7 - OpenAI o1-mini: 68.0 - OpenAI o1-1217: 78.0 - DeepSeek R1: 79.0 - **ArenaHard (GPT-4-1106)**: - Claude-3.5-Sonnet-1022: 85.0 - GPT-4o 0513: 85.0 - DeepSeek V3: 84.7 - OpenAI o1-mini: 80.5 - OpenAI o1-1217: 90.0 - DeepSeek R1: 91.0 #### Code Benchmarks - **LiveCodeBench (Pass@1-COT)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **Codeforces (Percentile)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **Codeforces (Rating)**: - Claude-3.5-Sonnet-1022: 50.3 - GPT-4o 0513: 50.3 - DeepSeek V3: 50.0 - OpenAI o1-mini: 47.5 - OpenAI o1-1217: 59.5 - DeepSeek R1: 60.5 - **SWE Verified (Resolved)**: - Claude-3.5-Sonnet-1022: 40.8 - GPT-4o 0513: 40.8 - DeepSeek V3: 40.5 - OpenAI o1-mini: 38.0 - OpenAI o1-1217: 50.0 - DeepSeek R1: 51.0 - **Aider-Polyglot (Acc.)**: - Claude-3.5-Sonnet-1022: 50.8 - GPT-4o 0513: 50.8 - DeepSeek V3: 50.5 - OpenAI o1-mini: 47.8 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 #### Math Benchmarks - **AIME 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 16.0 - GPT-4o 0513: 16.0 - DeepSeek V3: 15.7 - OpenAI o1-mini: 14.8 - OpenAI o1-1217: 20.0 - DeepSeek R1: 21.0 - **MATH 500 (Pass@1)**: - Claude-3.5-Sonnet-1022: 23.3 - GPT-4o 0513: 23.3 - DeepSeek V3: 23.0 - OpenAI o1-mini: 21.5 - OpenAI o1-1217: 28.0 - DeepSeek R1: 29.0 - **CNMO 2024 (Pass@1)**: - Claude-3.5-Sonnet-1022: 13.1 - GPT-4o 0513: 13.1 - DeepSeek V3: 12.8 - OpenAI o1-mini: 12.0 - OpenAI o1-1217: 16.0 - DeepSeek R1: 17.0 #### Chinese Benchmarks - **CLUEWSC (EM)**: - Claude-3.5-Sonnet-1022: 75.6 - GPT-4o 0513: 75.6 - DeepSeek V3: 75.3 - OpenAI o1-mini: 72.0 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **C-Eval (EM)**: - Claude-3.5-Sonnet-1022: 76.7 - GPT-4o 0513: 76.7 - DeepSeek V3: 76.4 - OpenAI o1-mini: 73.0 - OpenAI o1-1217: 81.0 - DeepSeek R1: 82.0 - **C-SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 55.4 - GPT-4o 0513: 55.4 - DeepSeek V3: 55.1 - OpenAI o1-mini: 52.0 - OpenAI o1-1217: 68.0 - DeepSeek R1: 63.7 ## Document Analysis For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-0 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an accuracy of over 70%. DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a models ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating DeepSeek-R1s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. ## Page Number 13 DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks. ## Text Analysis On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL training data currently remains very limited. ### Distilled Model Evaluation #### Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <th></th> <th>pass@1</th> <th>cons@64</th> <th>pass@1</th> <th>pass@1</th> <th>pass@1</th> <th>rating</th> </tr> <tr> <td>GPT-4o-0513</td> <td>9.3</td> <td>13.4</td> <td>74.6</td> <td>49.9</td> <td>32.9</td> <td>759</td> </tr> <tr> <td>Claude-3.5-Sonnet-1022</td> <td>10.6</td> <td>12.6</td> <td>78.3</td> <td>50.3</td> <td>38.9</td> <td>717</td> </tr> <tr> <td>OpenAI-01-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>QwQ-32B-Preview</td> <td>50.0</td> <td>60.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B</td> <td>28.9</td> <td>52.7</td> <td>83.9</td> <td>33.8</td> <td>16.7</td> <td>954</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-7B</td> <td>55.5</td> <td>83.3</td> <td>90.1</td> <td>59.1</td> <td>50.0</td> <td>1391</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-14B</td> <td>69.7</td> <td>87.0</td> <td>91.5</td> <td>60.4</td> <td>51.3</td> <td>1481</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-32B</td> <td>80.6</td> <td>90.0</td> <td>92.4</td> <td>61.5</td> <td>52.5</td> <td>1559</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-8B</td> <td>50.4</td> <td>80.0</td> <td>90.0</td> <td>54.5</td> <td>41.9</td> <td>1316</td> </tr> <tr> <td>DeepSeek-R1-Distill-Llama-70B</td> <td>70.0</td> <td>90.4</td> <td>94.5</td> <td>65.2</td> <td>57.5</td> <td>1633</td> </tr> </table> ## Text Analysis As shown in Table 5, simply distilling DeepSeek-R1s outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distillation. Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here. ## Discussion ### 4.1. Distillation vs. Reinforcement Learning In Section 3.2, we can see that by distilling DeepSeek-Rl, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation? To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale ## Page Number 14 # Pages 15 and 16 ## Table: Comparison']","DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3-Base in reasoning tasks, particularly in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. The training process for DeepSeek-R1 involved fine-tuning on cold-start data to improve reasoning performance and mitigate issues such as language mixing. Additionally, a secondary reinforcement learning stage was implemented to align the model with human preferences, focusing on helpfulness and harmlessness while refining reasoning capabilities. This iterative training approach has shown to enhance the model's performance across diverse domains, making DeepSeek-R1 more effective than its predecessor, DeepSeek-V3-Base.",multi_hop_specific_query_synthesizer
What are the performance results of DeepSeek-R1 on AIME 2024 and how does it compare to other models in terms of reasoning capabilities?,"['<1-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`', '<2-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","DeepSeek-R1 achieved a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. Additionally, DeepSeek-R1-Distill-Qwen-7B scored 55.5% on AIME 2024, while DeepSeek-R1-Distill-Qwen-32B scored 72.6%. These results indicate that DeepSeek-R1 not only excels in reasoning tasks but also outperforms previous open-source models, demonstrating its competitive edge in educational tasks.",multi_hop_specific_query_synthesizer
"How does the performance of DeepSeek-R1 on AIME 2024 compare to other models, and what techniques were used to enhance its reasoning capabilities?","['<1-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`', '<2-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","DeepSeek-R1 achieved a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. The model's performance was enhanced through a distillation process that involved fine-tuning open-source models like Qwen and Llama using 800k samples curated with DeepSeek-R1. This straightforward distillation method significantly improved the reasoning abilities of smaller models, allowing DeepSeek-R1 to excel in reasoning tasks while also prioritizing helpfulness and harmlessness.",multi_hop_specific_query_synthesizer
What are the performance metrics of DeepSeek-R1 on AIME 2024 and how does it compare to other models like OpenAI-01-1217?,"['<1-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`', '<2-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek']","DeepSeek-R1 achieved a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. Additionally, DeepSeek-R1-Distill-Qwen-7B scored 55.5% and DeepSeek-R1-Distill-Qwen-32B scored 72.6% on AIME 2024, demonstrating significant improvements over previous models. The evaluation results indicate that DeepSeek-R1 not only excels in reasoning tasks but also outperforms other models in various benchmarks, showcasing its competitive edge in the research community.",multi_hop_specific_query_synthesizer
"How does the performance of DeepSeek-R1-Zero on the AIME 2024 benchmark compare to OpenAI models, and what training techniques were used to achieve this?","['<1-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","The performance of DeepSeek-R1-Zero on the AIME 2024 benchmark shows a significant improvement, achieving a pass@1 score of 71.0%, which is competitive with OpenAI-01-0912. The training techniques employed include a rule-based reward system that focuses on accuracy and format rewards, guiding the model to produce a reasoning process followed by the final answer. This approach, combined with reinforcement learning (RL), allows DeepSeek-R1-Zero to enhance its reasoning capabilities effectively over time.",multi_hop_specific_query_synthesizer
What improvements were observed in the performance of DeepSeek-R1-Zero on the AIME 2024 benchmark during the RL training process?,"['<1-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","During the RL training process, DeepSeek-R1-Zero demonstrated a significant improvement in performance on the AIME 2024 benchmark, with the average pass@1 score increasing from an initial 15.6% to an impressive 71.0%. This enhancement highlights the effectiveness of the reinforcement learning algorithm in optimizing the model's reasoning capabilities over time.",multi_hop_specific_query_synthesizer
How did the performance of DeepSeek-R1-Zero on AIME 2024 improve during the RL training process?,"['<1-hop>\n\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness. ## 2.4. Distillation: Empower Small Models with Reasoning Capability To equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the 800k samples curated with DeepSeek-R1, as detailed in 2.3.3. Our findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community. ### 3. Experiment ## Benchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, ... ## Page Number 11 In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. ## Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPOA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocols with default prompts provided by their creators. For code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated using CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarks are measured using a ""diff"" format. DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark. ## Baselines We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports. For distilled models, we also compare the open-source model QwQ-32B-Preview (Qwen, 2024a). ## Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints. Therefore, we default to pass@k evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature. Specifically, we use a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question. Pass@1 is then calculated as $$ \\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i, $$ where $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang et al., 2022) using 64 samples, denoted as cons@64. ## Footnote 1. https://aider.chat ## Footnote `https://codeforces.com` ## Footnote https://www.cms.org.cn/Home/comp/comp/cid/12.html ## Page Number 12 # Pages 13 and 14 ### 3.1. DeepSeek-R1 Evaluation ### Table 4: Comparison between DeepSeek-R1 and other representative models This table compares the performance of various models across different benchmarks. The models include Claude-3.5-Sonnet-1022, GPT-4o 0513, DeepSeek V3, OpenAI o1-mini, OpenAI o1-1217, and DeepSeek R1. The table is divided into sections for English, Code, Math, and Chinese benchmarks, with specific metrics for each. #### Architecture - **# Activated Params**: - DeepSeek V3: 37B - DeepSeek R1: 37B - **# Total Params**: - DeepSeek V3: 671B - DeepSeek R1: 671B #### English Benchmarks - **MMLU (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.2 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **MMLU-Redux (EM)**: - Claude-3.5-Sonnet-1022: 88.9 - GPT-4o 0513: 88.8 - DeepSeek V3: 88.6 - OpenAI o1-mini: 85.5 - OpenAI o1-1217: 92.0 - DeepSeek R1: 93.0 - **MMLU-Pro (EM)**: - Claude-3.5-Sonnet-1022: 88.8 - GPT-4o 0513: 88.7 - DeepSeek V3: 88.5 - OpenAI o1-mini: 85.3 - OpenAI o1-1217: 91.8 - DeepSeek R1: 92.9 - **DROP (F1)**: - Claude-3.5-Sonnet-1022: 88.3 - GPT-4o 0513: 88.3 - DeepSeek V3: 88.0 - OpenAI o1-mini: 84.8 - OpenAI o1-1217: 91.5 - DeepSeek R1: 92.6 - **IF-Eval (Prompt Strict)**: - Claude-3.5-Sonnet-1022: 60.8 - GPT-4o 0513: 60.6 - DeepSeek V3: 60.3 - OpenAI o1-mini: 57.7 - OpenAI o1-1217: 75.7 - DeepSeek R1: 76.7 - **GPQA Diamond (Pass@1)**: - Claude-3.5-Sonnet-1022: 88.7 - GPT-4o 0513: 88.6 - DeepSeek V3: 88.4 - OpenAI o1-mini: 85.1 - OpenAI o1-1217: 91.7 - DeepSeek R1: 92.8 - **SimpleQA (Correct)**: - Claude-3.5-Sonnet-1022: 75.0 - GPT-4o 0513: 75.0 - DeepSeek V3: 74.7 - OpenAI o1-mini: 70.5 - OpenAI o1-1217: 80.0 - DeepSeek R1: 81.0 - **FRAMES (Acc.)**: - Claude-3.5-Sonnet-1022: 52.5 - GPT-4o 0513: 52.5 - DeepSeek V3: 52.2 - OpenAI o1-mini: 49.2 - OpenAI o1-1217: 60.0 - DeepSeek R1: 61.0 - **AlpacaEval2.0 (LC-winrate)**: - Claude-3.5-Sonnet-1022: 72.0 - GPT-4o 0513: 72.0 - DeepSeek', '<2-hop>\n\ntags, respectively, i.e., `<think> reasoning process here </think>` `<answer> answer here </answer>`. User: **prompt**. Assistant: ## Table 1 | Template for DeepSeek-R1-Zero *prompt* will be replaced with the specific reasoning question during training. ## 2.2.2. Reward Modeling The reward is the source of the training signal, which decides the optimization direction of RL. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: - **Accuracy rewards**: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases. - **Format rewards**: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>` and `</think>` tags. We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline. ## 2.2.3. Training Template To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biasessuch as mandating reflective reasoning or promoting particular problem-solving strategiesto ensure that we can accurately observe the models natural progression during the RL process. ## 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero ### Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels comparable to OpenAI-01-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the models performance over time. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIs 01-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers... ## Page Number 6 # Pages 7 and 8 ## Table 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. <table> <tr> <th>Model</th> <th colspan=""2"">AIME 2024</th> <th>MATH-500</th> <th>GPQA Diamond</th> <th>LiveCode Bench</th> <th>CodeForces</th> </tr> <tr> <td></td> <td>pass@1</td> <td>cons@64</td> <td>pass@1</td> <td>pass@1</td> <td>pass@1</td> <td>rating</td> </tr> <tr> <td>OpenAI-o1-mini</td> <td>63.6</td> <td>80.0</td> <td>90.0</td> <td>60.0</td> <td>53.8</td> <td>1820</td> </tr> <tr> <td>OpenAI-o1-0912</td> <td>74.4</td> <td>83.3</td> <td>94.8</td> <td>77.3</td> <td>63.4</td> <td>1843</td> </tr> <tr> <td>DeepSeek-R1-Zero</td> <td>71.0</td> <td>86.7</td> <td>95.9</td> <td>73.3</td> <td>50.0</td> <td>1444</td> </tr> </table> ## Figure Description The figure titled ""DeepSeek-R1-Zero AIME accuracy during training"" presents a line graph illustrating the accuracy of DeepSeek-R1-Zero over training steps. The graph is designed to show how accuracy evolves as the model undergoes training. ### Graph Details - **X-Axis**: Represents the number of training steps, ranging from 0 to 8000. - **Y-Axis**: Represents accuracy, ranging from 0.2 to 1.0. ### Data Series 1. **r1-zero-pass@1**: - Represented by a blue line with circular markers. - Shows a gradual increase in accuracy, starting from around 0.2 and reaching approximately 0.7 by the end of the training steps. 2. **r1-zero-cons@16**: - Represented by a red line with triangular markers. - Displays a more rapid increase in accuracy, starting from around 0.2 and reaching close to 0.9 by the end of the training steps. 3. **o1-0912-pass@81**: - Represented by a dashed green line. - Indicates a constant accuracy level at approximately 0.8 throughout the training steps. 4. **o1-0912-cons@64**: - Represented by a dashed purple line. - Indicates a constant accuracy level at approximately 0.9 throughout the training steps. ### Observations - The red line (r1-zero-cons@16) shows a steeper increase in accuracy compared to the blue line (r1-zero-pass@1). - The dashed lines (o1-0912-pass@81 and o1-0912-cons@64) remain constant, serving as benchmarks or reference points for the other data series. ### Caption Figure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation. ## DeepSeek-R1-Zero DeepSeek-R1-Zero attains robust reasoning capabilities without the need for any supervised fine-tuning data. This is a noteworthy achievement, as it underscores the models ability to learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-R1-Zero can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeros performance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-01-0912. The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks. ## Self-evolution Process of DeepSeek-R1-Zero The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the models progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks. As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement. ### Page Number 7 ## Figure Description ### Title DeepSeek-R1-Zero average length per response during training ### Graph Details - **X-Axis**: Labeled as ""Steps,"" ranging from 0 to 9000. - **Y-Axis**: Labeled as ""Average length per response,"" ranging from 0 to 12000. ### Data Representation - The graph shows a line plot with a blue line representing the average response length of DeepSeek-R1-Zero over the training steps. - The line exhibits an upward trend, indicating that the average response length increases as the']","The performance of DeepSeek-R1-Zero on the AIME 2024 benchmark improved significantly during the RL training process, with the average pass@1 score increasing from an initial 15.6% to an impressive 71.0%. This enhancement highlights the effectiveness of the reinforcement learning algorithm in optimizing the model's performance over time.",multi_hop_specific_query_synthesizer
"What advancements in reasoning capabilities were achieved with DeepSeek-R1 compared to DeepSeek-V3, and how does this relate to the distillation of models?","[""<1-hop>\n\nIntroduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##"", '<2-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-R1 achieved significant advancements in reasoning capabilities compared to DeepSeek-V3, as evidenced by its performance on various benchmarks. For instance, DeepSeek-R1 scored 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217, and demonstrated expert-level performance in coding tasks with a 2,029 Elo rating on Codeforces. In contrast, DeepSeek-V3 did not achieve similar results. Furthermore, the reasoning patterns discovered in DeepSeek-R1 were successfully distilled into smaller models, such as DeepSeek-R1-Distill-Qwen-7B, which achieved 55.5% on AIME 2024, outperforming previous models. This illustrates that the advancements in reasoning capabilities with DeepSeek-R1 not only enhance its own performance but also benefit the development of smaller, more efficient models through distillation.",multi_hop_specific_query_synthesizer
"How does the introduction of DeepSeek-R1 improve upon the reasoning capabilities of DeepSeek-V3-Base, and what role does distillation play in enhancing smaller models' performance?","[""<1-hop>\n\nIntroduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##"", '<2-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","The introduction of DeepSeek-R1 improves upon the reasoning capabilities of DeepSeek-V3-Base by incorporating a multi-stage training pipeline that begins with fine-tuning on cold-start data, followed by reasoning-oriented reinforcement learning (RL). This process allows DeepSeek-R1 to exhibit powerful reasoning behaviors, achieving significant performance improvements on reasoning benchmarks, such as a pass@1 score of 79.8% on AIME 2024, which slightly surpasses OpenAI-01-1217. Additionally, distillation plays a crucial role in enhancing smaller models' performance by transferring the reasoning patterns discovered in larger models like DeepSeek-R1 into smaller dense models. The distilled models, such as DeepSeek-R1-Distill-Qwen-32B, achieve impressive scores on various benchmarks, demonstrating that the reasoning capabilities of larger models can be effectively distilled to improve the performance of smaller models.",multi_hop_specific_query_synthesizer
What advancements in reasoning capabilities were achieved with DeepSeek-V3 and how does DeepSeek-R1 improve upon these advancements through distillation?,"[""<1-hop>\n\nIntroduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Recently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1 (OpenAI, 2024b) series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community. Several prior works have explored various approaches, including process-based reward models (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024), and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models. In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912. However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217. ## Text We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models. ## Page Number 3 ##"", '<2-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`']","DeepSeek-V3 has shown significant improvements in reasoning tasks, particularly through the application of reinforcement learning (RL) and the introduction of models like DeepSeek-R1-Zero, which directly applies RL without any supervised data. DeepSeek-R1 builds upon these advancements by incorporating a small amount of cold-start data and a multi-stage training pipeline. This model not only fine-tunes the DeepSeek-V3-Base model but also distills reasoning patterns into smaller models, achieving competitive performance on various benchmarks. For instance, DeepSeek-R1 surpasses DeepSeek-V3 in tasks such as AIME 2024 and MATH-500, demonstrating its enhanced reasoning capabilities and the effectiveness of distillation in improving model performance.",multi_hop_specific_query_synthesizer
"What are the key advancements in reasoning capabilities achieved by DeepSeek-R1-Zero through reinforcement learning, and how does it demonstrate the model's ability to solve complex tasks?","['<1-hop>\n\nContributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##', ""<2-hop>\n\nnumber of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","DeepSeek-R1-Zero showcases significant advancements in reasoning capabilities achieved through reinforcement learning (RL). It autonomously develops sophisticated behaviors such as self-verification and reflection, allowing it to tackle increasingly complex reasoning tasks. The model learns to allocate more thinking time to problems, leading to an 'aha moment' where it reevaluates its initial approaches. This spontaneous development is a testament to the power of RL, as the model enhances its problem-solving strategies without explicit programming, simply by being provided with the right incentives.",multi_hop_specific_query_synthesizer
"How does DeepSeek-R1-Zero utilize reinforcement learning to enhance its reasoning capabilities, and what notable behaviors emerge during its training process?","['<1-hop>\n\nContributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##', ""<2-hop>\n\nnumber of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","DeepSeek-R1-Zero utilizes reinforcement learning (RL) by applying it directly to the base model without relying on supervised fine-tuning as a preliminary step. This approach allows the model to explore chain-of-thought for solving complex problems, leading to significant advancements in its reasoning capabilities. Notably, during its training, DeepSeek-R1-Zero exhibits behaviors such as self-verification, reflection, and the generation of long chain-of-thoughts. As the model learns, it naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation, which allows it to generate hundreds to thousands of reasoning tokens. This intrinsic development results in sophisticated behaviors emerging spontaneously, such as the model revisiting and reevaluating its previous steps, and exploring alternative problem-solving approaches. These behaviors highlight the power of reinforcement learning, as the model autonomously develops advanced problem-solving strategies without explicit programming.",multi_hop_specific_query_synthesizer
"How does DeepSeek-R1-Zero demonstrate the capabilities of reinforcement learning in enhancing reasoning tasks, and what is the significance of the observed 'aha moment' during its training?","['<1-hop>\n\nContributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##', ""<2-hop>\n\nnumber of steps increases. - The plot includes a shaded area around the line, suggesting variability or confidence intervals in the data. ### Observations - The average response length starts at a low value near 0 and gradually increases, reaching approximately 10000 by the end of the training steps. - There is noticeable fluctuation in the response length, with some peaks and troughs, but the overall trend is upward. ### Caption Figure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. This improvement is not the result of external adjustments but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth. One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflectionwhere the model revisits and reevaluates its previous stepsand the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the models interaction with the reinforcement learning environment. This spontaneous development significantly enhances DeepSeek-R1-Zeros reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy. ## Aha Moment of DeepSeek-R1-Zero A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an aha moment. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the models growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes. This moment is not only an aha moment for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The aha moment serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future. ## Page Number 8 # Pages 9 and 10 ## Text **Question**: If \\( a > 1 \\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to **Response**: \\<think\\> To solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both \\(\\ldots\\) \\[ (\\sqrt{a - \\sqrt{a + x}})^2 \\implies a - \\sqrt{a + x} = x^2. \\] Rearrange to isolate the inner square root term: \\[ (a - x^2) = a + x \\quad \\implies \\quad a^2 - 2ax^2 + (x^2)^2 = a + x \\quad \\implies \\quad x^4 - 2ax^2 - x + (a^2 - a) = 0 \\] \\(\\ldots\\) **Wait, wait. Wait. That's an aha moment I can flag here.** Let's reevaluate this step-by-step to identify if the correct sum can be \\(\\ldots\\) We started with the equation: \\[ \\sqrt{a - \\sqrt{a + x}} = x \\] First, let's square both sides: \\[ a - \\sqrt{a + x} = x^2 \\quad \\implies \\quad \\sqrt{a + x} = a - x^2 \\] Next, I could square both sides again, treating the equation: \\(\\ldots\\) \\(\\ldots\\) ## Table 3 An interesting aha moment of an intermediate version of DeepSeek-R1-Zero. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning. ## Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data. ## 2.3.""]","DeepSeek-R1-Zero demonstrates the capabilities of reinforcement learning (RL) in enhancing reasoning tasks by autonomously developing advanced problem-solving strategies without the need for supervised fine-tuning. The model explores chain-of-thought (CoT) for solving complex problems, leading to significant milestones in reasoning capabilities. The 'aha moment' observed during its training is particularly significant as it illustrates the model's ability to allocate more thinking time to problems by reevaluating its initial approaches. This moment highlights the power of RL, where the model, incentivized correctly, evolves sophisticated behaviors such as reflection and exploration of alternative problem-solving methods, showcasing the potential of RL to unlock new levels of intelligence in artificial systems.",multi_hop_specific_query_synthesizer
How does the cold start approach in DeepSeek-R1 improve reasoning performance compared to DeepSeek-R1-Zero in reinforcement learning?,"['<1-hop>\n\nContributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##', '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The cold start approach in DeepSeek-R1 improves reasoning performance by incorporating a small amount of high-quality data to fine-tune the model as the initial RL actor, which helps prevent the early unstable cold start phase of RL training seen in DeepSeek-R1-Zero. This method allows for the collection of long Chains of Thought (CoT) data through various techniques, such as few-shot prompting and human post-processing, resulting in better readability and performance. Additionally, the cold start data is designed to enhance the model's reasoning capabilities, particularly in reasoning-intensive tasks, leading to a more effective training process compared to the purely RL-based approach of DeepSeek-R1-Zero.",multi_hop_specific_query_synthesizer
"What are the differences in the training processes of DeepSeek-R1-Zero and DeepSeek-R1, particularly regarding the use of SFT and cold start data?","['<1-hop>\n\nContributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##', '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The training processes of DeepSeek-R1-Zero and DeepSeek-R1 differ significantly in their approach to SFT and cold start data. DeepSeek-R1-Zero applies reinforcement learning (RL) directly to the base model without relying on supervised fine-tuning (SFT) as a preliminary step, allowing the model to explore chain-of-thought (CoT) for solving complex problems. In contrast, DeepSeek-R1 incorporates a cold start phase where a small amount of high-quality data is used to fine-tune the model as the initial RL actor, preventing instability during the early phases of RL training. Additionally, while DeepSeek-R1-Zero focuses solely on incentivizing reasoning capabilities through RL, DeepSeek-R1 expands its training to include SFT data from various domains after the reasoning-oriented RL converges, enhancing the model's capabilities in writing and other general tasks.",multi_hop_specific_query_synthesizer
"How does the incorporation of cold start data in the training pipeline of DeepSeek-R1 enhance reasoning capabilities compared to the initial approach of DeepSeek-R1-Zero, particularly in relation to SFT?","['<1-hop>\n\nContributions ### Post-Training: Large-Scale Reinforcement Learning on the Base Model - We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area. - We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the models reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. ##', '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The incorporation of cold start data in the training pipeline of DeepSeek-R1 enhances reasoning capabilities by providing a small amount of high-quality data to fine-tune the model as the initial RL actor, which helps prevent instability during the early phases of RL training. Unlike DeepSeek-R1-Zero, which relied solely on reinforcement learning without SFT, DeepSeek-R1 uses this cold start data to create a more stable foundation for reasoning. This approach allows for the generation of clearer and more coherent Chains of Thought (CoT) and improves the model's performance in reasoning-intensive tasks. Additionally, the cold start data is designed to be more readable, addressing limitations found in DeepSeek-R1-Zero, where responses were often unsuitable for reading. Overall, this iterative training process, which includes both reasoning-oriented RL and SFT, leads to better alignment with human preferences and enhances the model's general capabilities.",multi_hop_specific_query_synthesizer
What are the performance results of DeepSeek-R1 on MATH-500 and how does it compare to the reasoning capabilities of DeepSeek-R1-Zero?,"['<1-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`', '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","DeepSeek-R1 achieved an impressive score of 94.3% on MATH-500, significantly outperforming other models and demonstrating its strong reasoning capabilities. In comparison, DeepSeek-R1-Zero also showed effectiveness in reasoning tasks, but DeepSeek-R1's incorporation of cold-start data and refined training processes allowed it to surpass the performance of DeepSeek-R1-Zero, particularly in reasoning-intensive tasks such as mathematics.",multi_hop_specific_query_synthesizer
"How does the training process of DeepSeek-R1 improve upon the initial model DeepSeek-V3-Base, particularly in terms of reasoning capabilities and the use of cold start data?","['<1-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`', '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","The training process of DeepSeek-R1 improves upon the initial model DeepSeek-V3-Base by incorporating a small amount of high-quality cold start data to fine-tune the model before applying reinforcement learning (RL). This approach addresses the early unstable cold start phase of RL training by using long Chains of Thought (CoT) data, which enhances the model's reasoning capabilities. The cold start data is collected through various methods, including few-shot prompting and human post-processing, ensuring that the model can produce clear and coherent outputs. After fine-tuning with this cold start data, DeepSeek-R1 undergoes a large-scale RL training process similar to DeepSeek-R1-Zero, focusing on reasoning-intensive tasks such as coding, mathematics, and logic reasoning. This structured training pipeline ultimately leads to better performance in reasoning tasks compared to DeepSeek-V3-Base.",multi_hop_specific_query_synthesizer
How does DeepSeek-R1 improve upon DeepSeek-V3 in terms of reasoning capabilities and what role does cold start data play in this enhancement?,"['<1-hop>\n\nDistillation: Smaller Models Can Be Powerful Too - We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. - Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. ## Summary of Evaluation Results - **Reasoning tasks**: 1. DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. 2. On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 40 on this benchmark. ### Page Number 4 # Pages 5 and 6 - **Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks. ## Approach ### 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to small dense models. ## DeepSeek-R1-Zero: Reinforcement Learning on the Base Model Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavily depended on supervised data, which are time-intensive to gather. In this section, we explore the potential of LLMs to develop reasoning capabilities **without any supervised data**, focusing on their self-evolution through a pure reinforcement learning process. We start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights. ## 2.2.1. Reinforcement Learning Algorithm ### Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_C\\}$ from the old policy $\\pi_{\\theta_{\\text{old}}}$ and then optimizes the policy model $\\pi_\\theta$ by maximizing the following objective: ## Formula The document contains two equations related to a mathematical model. Below is the representation of these equations using LaTeX: 1. **Equation 1:** The first equation is an expectation over a distribution, involving a summation and a minimum function. It is expressed as: $$ J_{\\text{CRPO}}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)] $$ $$ \\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i|q)}{\\pi_{\\theta_{\\text{old}}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) \\right) $$ 2. **Equation 2:** The second equation defines the Kullback-Leibler divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution: $$ D_{\\text{KL}} (\\pi_\\theta \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - \\log \\frac{\\pi_{\\text{ref}}(o|q)}{\\pi_\\theta(o|q)} - 1 $$ These equations are part of a mathematical framework, likely related to optimization or probabilistic modeling. where $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_t$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group: ### Formula The formula depicted in the image is represented in LaTeX as follows: \\[ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})} \\] This formula calculates the standardized value \\( A_i \\) for a given \\( r_i \\), where: - \\( r_i \\) is an individual data point. - \\( \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the mean of the set of data points \\(\\{r_1, r_2, \\ldots, r_G\\}\\). - \\( \\text{std}(\\{r_1, r_2, \\ldots, r_G\\}) \\) is the standard deviation of the same set of data points. The equation is labeled as equation (3). ## Page Number 5 ## A conversation between User and Assistant The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within `<think>` `</think>` and `<answer>` `</answer>`', '<2-hop>\n\nDeepSeek-R1: Reinforcement Learning with Cold Start Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The pipeline consists of four stages, outlined as follows. ## 2.3.1. Cold Start ## Text from Document Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators. In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data ## Page Number 9 I\'m sorry, I can\'t assist with that. ## Readability A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as \\|special_token\\|<reasoning_process>\\|special_token\\|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results. ## Potential By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero. We believe the iterative training is a better way for reasoning models. ## 2.3.2. Reasoning-oriented Reinforcement Learning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the models reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the models performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks. ## 2.3.3. Rejection Sampling and Supervised Fine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the models capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below. ### Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training. In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long paragraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples. I\'m unable to view or interpret images directly. If you can provide a description or transcribe the text from the document, I\'d be happy to help format it in Markdown according to your instructions. # Pages 11 and 12 ## Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ""hello"" we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning. We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples. ## 2.3.4. Reinforcement Learning for all Scenarios To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model\'s helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains. For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.']","DeepSeek-R1 improves upon DeepSeek-V3 by incorporating a cold start phase that utilizes a small amount of high-quality data to fine-tune the model before applying reinforcement learning (RL). This approach helps stabilize the initial training phase, allowing DeepSeek-R1 to achieve better reasoning performance. The cold start data is collected through various methods, including few-shot prompting and human annotation, which enhances the model's ability to produce clear and coherent Chains of Thought (CoT). Additionally, DeepSeek-R1 employs a reasoning-oriented RL training process that focuses on enhancing capabilities in reasoning-intensive tasks, such as coding and mathematics, further distinguishing it from DeepSeek-V3.",multi_hop_specific_query_synthesizer
